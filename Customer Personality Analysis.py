# -*- coding: utf-8 -*-
"""Machine learning final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gnLm8ZebLNtcxZxafVIs3x52VtvUTIIA

# **Enhancing Marketing Strategies with Customer Personality Analysis**

# Problem Statement

The problem we propose to solve is to enhance marketing strategies by leveraging customer
personality analysis. Traditional marketing approaches often rely on generic segmentation
based on demographics or historical purchase behavior. However, this approach fails
to capture the nuances of individual customer preferences and behaviors, limiting the
effectiveness of marketing campaigns. By leveraging customer personality analysis, we
aim to develop a machine learning algorithm model that will target specific customers
and personalize the marketing approach.

# Dataset Description

The dataset Customer Personality Analysis from Kaggle provides valuable insights into
customer behavior and preferences. It contains information about customers, including
their purchasing history. The dataset is a comprehensive collection of customer attributes,
providing a rich source of information for understanding and analyzing customer behavior.

# Data preparation and preprocessing
"""

# Import libraries
import sys
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import colors
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from yellowbrick.cluster import KElbowVisualizer

if not sys.warnoptions:
    warnings.simplefilter("ignore")
np.random.seed(42)

# Load dataset into DataFrame data
dataset = pd.read_csv('marketing_campaign.csv', sep="\t")

# Check number of columns and rows
dataset.shape

"""This dataset contains 29 variables and 2240 observations about different customers."""

# Check columns name
dataset.columns

# Look for description of number of values in each column
dataset.info()

"""The Income variable has 2016 where other has 2040 values, which means in Income variable there is missing values, then we have to fill it with NA"""

# Fill NA where there is missing values in Income variable
dataset['Income'] = dataset['Income'].fillna(dataset['Income'].mean())

# Let's check whether there are no any other missing values
dataset.isnull().sum()

# Check for duplicate
dataset.duplicated().sum()

dataset.head(2)

"""From the above output we can see that there are some variable that need some data cleaning, we are going to modify some features as well as create new ones for further analysis and modeling

Age: The age of the customers are the age in 2014 as it's the last record we have (6th Dec 2014) or we can round that up to 2023

Marital_Status: Narrowing down to 2 categories

Total_Children: Merging Kidhome and Teenhome columns into 1 column which describes the number of children living in the household

Dt_Customer: Extracting new features out of dates to make Day, Dayofweek, Month, and Year features

Education: Narrowing down to 3 categories

Is_Parent: Referring to the parenthood status

Total_Spent: Customer's total spent on products

# Feature engineering
"""

# Round age up to 2023
dataset['Age'] = 2023 - dataset["Year_Birth"]

dataset['Age'].max()

"""possible outliers in Age variable"""

# Let's classify values in Marital status variable into two categories: Relationship and Single
dataset['Marital_Status'] = dataset['Marital_Status'].replace(['Married', 'Together'],'Relationship')
dataset['Marital_Status'] = dataset['Marital_Status'].replace(['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd'],'Single')

dataset['Marital_Status'].value_counts()

from pandas.core.internals.construction import dataclasses_to_dicts
# Let's merge some features which are in same category into one single column
# All Kidhome and Teenhome will be into one single feature ChildrenHome
dataset['ChildrenHome'] = dataset['Kidhome'] + dataset['Teenhome']
# All amount spent in MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts and MntGoldProds into one single feature AmountSpent
dataset['AmountSpent'] = dataset['MntWines'] + dataset['MntFruits'] + dataset['MntMeatProducts'] + dataset['MntFishProducts'] + dataset['MntSweetProducts'] + dataset['MntGoldProds']
# All promotion made and accepted in AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5 and Response into one single feature TotalAcceptedCmp
dataset['TotalAcceptedCmp'] = dataset['AcceptedCmp1'] + dataset['AcceptedCmp2'] + dataset['AcceptedCmp3'] + dataset['AcceptedCmp4'] + dataset['AcceptedCmp5'] + dataset['Response']
# All purchases in NumWebPurchases, NumCatalogPurchases, NumStorePurchases and NumDealsPurchases into one single feature NumTotalPurchases
dataset['NumTotalPurchases'] = dataset['NumWebPurchases'] + dataset['NumCatalogPurchases'] + dataset['NumStorePurchases'] + dataset['NumDealsPurchases']

dataset['ChildrenHome'].value_counts()

dataset['AmountSpent'].max()

dataset['TotalAcceptedCmp'].max()

dataset['NumTotalPurchases'].max()

# parse Dt_Customer values into DateTime format
dataset['Dt_Customer'] = pd.to_datetime(dataset.Dt_Customer)
# Initialize the first day to be able to count days of customers engagement
dataset['first_day'] = '01-01-2023'
# Convert First_day value into DateTime format
dataset['first_day'] = pd.to_datetime(dataset.first_day)
# Count days of customer engagement
dataset['DaysEngaged'] = (dataset['first_day'] - dataset['Dt_Customer']).dt.days

dataset['DaysEngaged'].min()

# Let's classify values in Education variable into two categories: Post Graduate and Under Graduate
dataset['Education'] = dataset['Education'].replace(['PhD','2n Cycle','Graduation', 'Master'],'Post Graduate')
dataset['Education'] = dataset['Education'].replace(['Basic'], 'Under Graduate')

dataset['Education'].value_counts()

# For clarity, renaming some columns
dataset = dataset.rename(columns={"MntWines": "Wines", "MntFruits": "Fruits", "MntMeatProducts": "Meat", "MntSweetProducts": "Sweets", "MntGoldProds": "Gold"})

# Feature pertaining to parenthood
dataset["Is_Parent"] = np.where(dataset["ChildrenHome"] > 0, 1, 0)

# Dropping some redundant features
to_drop = ["Dt_Customer", "Z_CostContact", "Z_Revenue"]
dataset = dataset.drop(to_drop, axis=1)

dataset.describe()

# Initializing Color pallets
sns.set(rc={"axes.facecolor": "#edf9ff", "figure.facecolor": "#edf9ff"})
pallet = ["#2f2f68", "#6f729e", "#b1b2d6", "#c9c0b9", "#788a9f", "#60abf3"]
cmap = colors.ListedColormap(["#2f2f68", "#6f729e", "#D6B2B1", "#b1b2d6"])
pal = ["#2f2f68", "#c9c0b9", "#788a9f", "#60acf3"]

# Assuming you have a DataFrame called 'data' with the required columns

To_Plot = ["Income", "Recency", "DaysEngaged", "Age", "AmountSpent", "Is_Parent"]

print("Relative Plot Of Some Selected Features: A Data Subset")
plt.figure()
sns.pairplot(dataset[To_Plot], hue="Is_Parent", palette=["#2f2f68", "#60abf3"])
plt.show()

# Assuming you have a DataFrame called 'data' with the required columns

# Dropping the outliers by setting a cap on Age and Income
dataset = dataset[(dataset["Age"] < 90)]
dataset = dataset[(dataset["Income"] < 600000)]

print(f"The total data points after removing the outliers are: {len(dataset)}")

#correlation matrix
cor_mat = dataset.corr()
plt.figure(figsize=(20, 20))
sns.heatmap(cor_mat, annot=True, cmap=cmap, center=0)

# Get list of categorical variables
s = (dataset.dtypes == 'object')
object_cols = list(s[s].index)
print("Categorical variables in the dataset:", object_cols)

from sklearn.preprocessing import LabelEncoder

# Assuming you have a DataFrame called 'data' with categorical columns

LE = LabelEncoder()
object_cols = dataset.select_dtypes(include=['object']).columns

for col in object_cols:
    dataset[col] = dataset[col].astype(str)  # Convert column to string
    dataset[col] = LE.fit_transform(dataset[col])

print("All features are now numerical.")

from sklearn.preprocessing import StandardScaler

# Assuming you have a DataFrame called 'data_copy' with the required columns

# Creating a copy of the data
data_copy = dataset.copy()

# Creating a subset of the DataFrame by dropping the features on deals accepted and promotions
cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','first_day']
data_copy = data_copy.drop(cols_del, axis=1)

# Scaling the data
scaler = StandardScaler()
scaler.fit(data_copy)
scaled_data = pd.DataFrame(scaler.transform(data_copy), columns=data_copy.columns)

print("All features are now scaled using StandardScaler.")

from sklearn.decomposition import PCA

# Assuming you have a DataFrame called 'scaled_ds' with the required columns

# Initiating PCA to reduce dimensions (features) to 3
pca = PCA(n_components=3)
pca.fit(scaled_data)
pca_ds = pd.DataFrame(pca.transform(scaled_data), columns=["col1", "col2", "col3"])

# Describing the PCA transformed dataset
pca_ds.describe().T

# A 3D Projection Of Data In The Reduced Dimension
x = pca_ds["col1"]
y = pca_ds["col2"]
z = pca_ds["col3"]
# Plotting
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection="3d")
ax.scatter(x, y, z, c="blue", marker="o")
ax.set_title("A 3D Projection Of Data In The Reduced Dimension")
plt.show()

# Quick examination of elbow method to find numbers of clusters to make.
print('Elbow Method to determine the number of clusters to be formed:')
Elbow_M = KElbowVisualizer(KMeans(), k=10)
Elbow_M.fit(pca_ds)
Elbow_M.show()

#Initiating the Agglomerative Clustering model
AC = AgglomerativeClustering(n_clusters=4)
# fit model and predict clusters
yhat_AC = AC.fit_predict(pca_ds)
pca_ds["Clusters"] = yhat_AC
#Adding the Clusters feature to the orignal dataframe.
dataset["Clusters"] = yhat_AC

#Plotting the clusters
fig = plt.figure(figsize=(10, 8))
ax = plt.subplot(111, projection='3d', label="bla")
ax.scatter(x, y, z, s=40, c=pca_ds["Clusters"], marker='o', cmap=cmap)
ax.set_title("The Plot Of The Clusters")
plt.show()

fig = sns.countplot(x=dataset["Clusters"], palette=pal)
fig.set_title("Distribution Of The Clusters")
plt.show()

fig = sns.scatterplot(data=dataset, x=dataset["AmountSpent"], y=dataset["Income"], hue=dataset["Clusters"])
fig.set_title("Profile of a group of individuals based on their income and spending habits")
plt.legend()
plt.show()

plt.figure()
fig = sns.swarmplot(x=dataset["Clusters"], y=dataset["AmountSpent"], color="#CBEDDD",)
fig = sns.boxenplot(x=dataset["Clusters"], y=dataset["AmountSpent"], palette=pal)
plt.show()

#Creating a feature to get a sum of accepted promotions
dataset["Total_Promos"] = dataset["AcceptedCmp1"] + dataset["AcceptedCmp2"] + dataset["AcceptedCmp5"]
#Plotting count of total campaign accepted.
plt.figure()
fig = sns.countplot(x=dataset["Total_Promos"], hue=dataset["Clusters"], palette=pal)
fig.set_title("Count Of Promotion Accepted")
fig.set_xlabel("Number Of Total Accepted Promotions")
plt.show()

#Plotting the number of deals purchased
plt.figure()
pl = sns.boxenplot(y=dataset["NumDealsPurchases"], x=dataset["Clusters"], palette=pal)
pl.set_title("Number of Deals Purchased")
plt.show()

# Assuming you have a DataFrame called 'data' with the required columns

purchases = ["NumWebPurchases", "NumCatalogPurchases", "NumStorePurchases"]

for i in purchases:
    plt.figure()
    sns.jointplot(x=dataset[i], y=dataset["AmountSpent"], hue=dataset["Clusters"], palette=pal)
    plt.show()

# Assuming you have a DataFrame called 'data' with the required columns

personal_info = ["Kidhome", "Teenhome", "DaysEngaged", "Age", "ChildrenHome", "Marital_Status"]

for i in personal_info:
    plt.figure()
    sns.jointplot(x=dataset[i], y=dataset["AmountSpent"], hue=dataset["Clusters"], kind="scatter")
    plt.show()